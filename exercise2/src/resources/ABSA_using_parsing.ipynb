{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/traindata.csv\",sep = \"\\t\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "dep_parser = CoreNLPDependencyParser(url='http://localhost:9000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_link(target_word,parse,dep):\n",
    "    for i in range(len(parse)):\n",
    "        if (target_word in u[i][2][0]):\n",
    "            dep.append(u[i][0][0])\n",
    "        elif (target_word in u[i][0][0]):\n",
    "            dep.append(u[i][2][0])\n",
    "    return dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_second_link(target_word,parse,dep):\n",
    "    for i in range(len(parse)):\n",
    "        if (target_word in u[i][2][0]) & (\"NN\" not in u[i][0][1] ) & (\"DT\" not in u[i][0][1]):\n",
    "            dep.append(u[i][0][0])\n",
    "        elif (target_word in u[i][0][0]) & (\"NN\" not in u[i][2][1]) & ((\"DT\" not in u[i][2][1])):\n",
    "            dep.append(u[i][2][0])\n",
    "    return dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deps = []\n",
    "for j in range(len(df)):\n",
    "    try:\n",
    "        current_dep = []\n",
    "        parses = dep_parser.parse(df.iloc[j,4].split())\n",
    "        context = df.iloc[j,2].split()\n",
    "    #parses_list = list(dep_parser.parse(df.iloc[5,4].split()))\n",
    "        u= [[(governor, dep, dependent) for governor, dep, dependent in parse.triples()] for parse in parses][0]\n",
    "    except:\n",
    "        print(j)\n",
    "    for word in context: \n",
    "        get_first_link(word,u,current_dep)\n",
    "    direct_links = len(current_dep)\n",
    "    for i in range(direct_links):\n",
    "        context = current_dep[i]\n",
    "        get_second_link(context,u,current_dep)\n",
    "    deps.append(list(set(current_dep)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sentence(dependencies,sentence):\n",
    "    sent = sentence.split()\n",
    "    final_sent = \"\"\n",
    "    for i in range(len(sent)):\n",
    "        if sent[i] in dependencies:\n",
    "            final_sent+=sent[i]+\" \"\n",
    "    return final_sent\n",
    "#Rajouter tous les mots entre les deux????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.iloc[:,4] = df.iloc[:,4].str.replace(r'[^\\w\\s]+', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df[\"terms\"] = deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"new_sent\"] = df.apply(lambda x : filter_sentence(x[\"terms\"],x[4]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(sent):\n",
    "    labels = []\n",
    "    for i in range(len(sent)):\n",
    "        if(sent[i]==\"positive\"):\n",
    "            labels.append(0)\n",
    "        elif(sent[i]==\"negative\"):\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(2)\n",
    "    return torch.tensor(labels).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AdamW\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "#encoding = tokenizer(list(df_test.iloc[:,-1]), return_tensors='pt', padding=\"longest\", truncation=True)\n",
    "\n",
    "train_encodings = tokenizer(list(df_train.iloc[:,-1]), return_tensors='pt', padding=\"longest\", truncation=True)\n",
    "train_labels = get_labels(list(df_train.iloc[:,0]))\n",
    "\n",
    "val_encodings = tokenizer(list(df_test.iloc[:,-1]), return_tensors='pt', padding=\"longest\",truncation=True)\n",
    "val_labels = get_labels(list(df_test.iloc[:,0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataLoader(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings,labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.encodings['input_ids'][idx]\n",
    "        attention_mask = self.encodings[\"attention_mask\"][idx]\n",
    "        label = self.labels[0][idx]\n",
    "        return input_ids, attention_mask,label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DataLoader(train_encodings,train_labels)\n",
    "val_dataset = DataLoader(val_encodings,val_labels)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size= 16, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size= 16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel \n",
    "bert = BertModel.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(768,156)\n",
    "        self.fc2 = nn.Linear(156,3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.softmax(x,dim=1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "\n",
    "## Train the model and validate it after each epoch.\n",
    "## Provide the train-val loss graph.\n",
    "total_train_losses = []\n",
    "total_train_acc= []\n",
    "\n",
    "total_val_losses = []\n",
    "total_val_acc = []\n",
    "\n",
    "# *****START CODE\n",
    "for epoch in range(1,epochs+1):\n",
    "    ##TRAINING##\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    train_acc = []\n",
    "\n",
    "\n",
    "\n",
    "    for batch in train_dataloader:        \n",
    "        input_ids_batch, attention_mask_batch, lbl_batch = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        bert_rep = bert(input_ids_batch, attention_mask=attention_mask_batch)[0][:,0,:]\n",
    "        outputs = model(bert_rep)\n",
    "        loss = criterion(outputs,lbl_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        pred = outputs.argmax(axis=1)\n",
    "        train_acc.append(accuracy_score(lbl_batch,pred))\n",
    "        \n",
    "\n",
    "        \n",
    "    train_loss_mean = np.mean(train_losses)\n",
    "    total_train_losses.append(train_loss_mean)\n",
    "    \n",
    "    train_acc_mean = np.mean(train_acc)\n",
    "    total_train_acc.append(train_acc_mean)\n",
    "    # VALIDATION\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    validation_acc = []\n",
    "    \n",
    "    for val_batch in val_dataloader:\n",
    "        input_ids_batch, attention_mask_batch, lbl_batch = val_batch\n",
    "        bert_rep = bert(input_ids_batch, attention_mask=attention_mask_batch)[0][:,0,:]\n",
    "        outputs = model(bert_rep)\n",
    "        loss = criterion(outputs,lbl_batch)\n",
    "        val_losses.append(loss.item())\n",
    "        \n",
    "        pred = outputs.argmax(axis=1)\n",
    "        validation_acc.append(accuracy_score(lbl_batch,pred))\n",
    "        \n",
    "    \n",
    "    val_loss_mean = np.mean(val_losses)\n",
    "    total_val_losses.append(val_loss_mean)\n",
    "    \n",
    "    val_acc_mean = np.mean(validation_acc)\n",
    "    total_val_acc.append(val_acc_mean)\n",
    "    \n",
    "    print('Epoch {}/{}): Train Loss: {:.6f} \\t Validation Loss: {:.6f}'.format(\n",
    "            epoch, epochs, train_loss_mean, val_loss_mean))\n",
    "    print('Epoch {}/{}): Train Acc: {:.6f} \\t Validation Acc: {:.6f}'.format(\n",
    "            epoch, epochs, train_acc_mean, val_acc_mean))\n",
    "    \n",
    "    \n",
    "    \n",
    "# *****END CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"data/devdata.csv\",sep = \"\\t\",header=None)\n",
    "\n",
    "deps_test = []\n",
    "for j in range(len(test)):\n",
    "    try:\n",
    "        current_dep = []\n",
    "        parses = dep_parser.parse(test.iloc[j,4].split())\n",
    "        context = test.iloc[j,2].split()\n",
    "    #parses_list = list(dep_parser.parse(df.iloc[5,4].split()))\n",
    "        u= [[(governor, dep, dependent) for governor, dep, dependent in parse.triples()] for parse in parses][0]\n",
    "    except:\n",
    "        pass\n",
    "    for word in context: \n",
    "        get_first_link(word,u,current_dep)\n",
    "    direct_links = len(current_dep)\n",
    "    for i in range(direct_links):\n",
    "        context = current_dep[i]\n",
    "        get_second_link(context,u,current_dep)\n",
    "    deps_test.append(list(set(current_dep)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.iloc[:,4] = test.iloc[:,4].str.replace(r'[^\\w\\s]+', ' ')\n",
    "test[\"terms\"] = deps_test\n",
    "test[\"new_sent\"] = test.apply(lambda x : filter_sentence(x[\"terms\"],x[4]),axis=1)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "#encoding = tokenizer(list(df_test.iloc[:,-1]), return_tensors='pt', padding=\"longest\", truncation=True)\n",
    "\n",
    "test_encodings = tokenizer(list(test.iloc[:,-1]), return_tensors='pt', padding=\"longest\", truncation=True)\n",
    "test_labels = get_labels(list(test.iloc[:,0]))[0]\n",
    "\n",
    "bert_rep = bert(test_encodings[\"input_ids\"], test_encodings[\"attention_mask\"])[0][:,0,:]\n",
    "outputs = model(bert_rep)\n",
    "test_pred = np.array(outputs.argmax(axis=1))\n",
    "accuracy = accuracy_score(test_labels,test_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
